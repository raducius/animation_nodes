{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raducius/animation_nodes/blob/master/demo/MMPose_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F77yOqgkX8p4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/open-mmlab/mmpose/blob/master/demo/MMPose_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_h0e90xzw0w"
      },
      "source": [
        "# MMPose Tutorial\n",
        "\n",
        "Welcome to MMPose colab tutorial! In this tutorial, we will show you how to\n",
        "- perform inference with an MMPose model\n",
        "- train a new mmpose model with your own datasets\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMVTUneIzw0x"
      },
      "source": [
        "## Install MMPose\n",
        "\n",
        "We recommend to use a conda environment to install mmpose and its dependencies. And compilers `nvcc` and `gcc` are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvKWH89zw0x",
        "outputId": "f94a25b5-ac43-4b6f-fc85-d9a8f175dc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "/usr/local/bin/python\n"
          ]
        }
      ],
      "source": [
        "# check NVCC version\n",
        "!nvcc -V\n",
        "\n",
        "# check GCC version\n",
        "!gcc --version\n",
        "\n",
        "# check python in conda environment\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26-3yY31zw0y",
        "outputId": "f62f3e96-4853-44c6-ef2a-4462cb8de0c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu111 (from versions: 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0, 2.3.1, 2.3.1+cpu, 2.3.1+cpu.cxx11.abi, 2.3.1+cu118, 2.3.1+cu121, 2.3.1+rocm5.7, 2.3.1+rocm6.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu111\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n",
            "Collecting mmcv-full\n",
            "  Downloading mmcv-full-1.7.2.tar.gz (607 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from mmcv-full)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mmcv-full) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mmcv-full) (25.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from mmcv-full) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from mmcv-full) (6.0.3)\n",
            "Collecting yapf (from mmcv-full)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->mmcv-full) (4.5.0)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv-full\n"
          ]
        }
      ],
      "source": [
        "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "%pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "%pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n",
        "\n",
        "# install mmdet for inference demo\n",
        "%pip install mmdet\n",
        "\n",
        "# clone mmpose repo\n",
        "%rm -rf mmpose\n",
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "%cd mmpose\n",
        "\n",
        "# install mmpose dependencies\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "# install mmpose in develop mode\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIEhiA44zw0y"
      },
      "outputs": [],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "\n",
        "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
        "print('torchvision version:', torchvision.__version__)\n",
        "\n",
        "# Check MMPose installation\n",
        "import mmpose\n",
        "\n",
        "print('mmpose version:', mmpose.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "print('cuda version:', get_compiling_cuda_version())\n",
        "print('compiler information:', get_compiler_version())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyrovOnDzw0z"
      },
      "source": [
        "## Inference with an MMPose model\n",
        "\n",
        "MMPose provides high level APIs for model inference and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaUNCi28zw0z"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
        "                         vis_pose_result, process_mmdet_results)\n",
        "from mmdet.apis import inference_detector, init_detector\n",
        "\n",
        "local_runtime = False\n",
        "\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow  # for image visualization in colab\n",
        "except:\n",
        "    local_runtime = True\n",
        "\n",
        "pose_config = 'configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
        "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
        "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "\n",
        "# initialize pose model\n",
        "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
        "# initialize detector\n",
        "det_model = init_detector(det_config, det_checkpoint)\n",
        "\n",
        "img = 'tests/data/coco/000000196141.jpg'\n",
        "\n",
        "# inference detection\n",
        "mmdet_results = inference_detector(det_model, img)\n",
        "\n",
        "# extract person (COCO_ID=1) bounding boxes from the detection results\n",
        "person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
        "\n",
        "# inference pose\n",
        "pose_results, returned_outputs = inference_top_down_pose_model(\n",
        "    pose_model,\n",
        "    img,\n",
        "    person_results,\n",
        "    bbox_thr=0.3,\n",
        "    format='xyxy',\n",
        "    dataset=pose_model.cfg.data.test.type)\n",
        "\n",
        "# show pose estimation results\n",
        "vis_result = vis_pose_result(\n",
        "    pose_model,\n",
        "    img,\n",
        "    pose_results,\n",
        "    dataset=pose_model.cfg.data.test.type,\n",
        "    show=False)\n",
        "# reduce image size\n",
        "vis_result = cv2.resize(vis_result, dsize=None, fx=0.5, fy=0.5)\n",
        "\n",
        "if local_runtime:\n",
        "    from IPython.display import Image, display\n",
        "    import tempfile\n",
        "    import os.path as osp\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file_name = osp.join(tmpdir, 'pose_results.png')\n",
        "        cv2.imwrite(file_name, vis_result)\n",
        "        display(Image(file_name))\n",
        "else:\n",
        "    cv2_imshow(vis_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOulhU_Wsr_S"
      },
      "source": [
        "## Train a pose estimation model on a customized dataset\n",
        "\n",
        "To train a model on a customized dataset with MMPose, there are usually three steps:\n",
        "1. Support the dataset in MMPose\n",
        "1. Create a config\n",
        "1. Perform training and evaluation\n",
        "\n",
        "### Add a new dataset\n",
        "\n",
        "There are two methods to support a customized dataset in MMPose. The first one is to convert the data to a supported format (e.g. COCO) and use the corresponding dataset class (e.g. TopdownCOCODataset), as described in the [document](https://mmpose.readthedocs.io/en/latest/tutorials/2_new_dataset.html#reorganize-dataset-to-existing-format). The second one is to add a new dataset class. In this tutorial, we give an example of the second method.\n",
        "\n",
        "We first download the demo dataset, which contains 100 samples (75 for training and 25 for validation) selected from COCO train2017 dataset. The annotations are stored in a different format from the original COCO format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlSP8JNr9pEr"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "%mkdir data\n",
        "%cd data\n",
        "!wget https://download.openmmlab.com/mmpose/datasets/coco_tiny.tar\n",
        "!tar -xf coco_tiny.tar\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDzqo6pwB-Zz"
      },
      "outputs": [],
      "source": [
        "# check the directory structure\n",
        "!apt-get -q install tree\n",
        "!tree data/coco_tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef-045CUCdb3"
      },
      "outputs": [],
      "source": [
        "# check the annotation format\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "anns = json.load(open('data/coco_tiny/train.json'))\n",
        "\n",
        "print(type(anns), len(anns))\n",
        "pprint.pprint(anns[0], compact=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4Dt1io8D7m8"
      },
      "source": [
        "After downloading the data, we implement a new dataset class to load data samples for model training and validation. Assume that we are going to train a top-down pose estimation model (refer to [Top-down Pose Estimation](https://github.com/open-mmlab/mmpose/tree/master/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap#readme) for a brief introduction), the new dataset class inherits `TopDownBaseDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR9ZVXuPFy4v"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os.path as osp\n",
        "from collections import OrderedDict\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from mmpose.core.evaluation.top_down_eval import (keypoint_nme,\n",
        "                                                  keypoint_pck_accuracy)\n",
        "from mmpose.datasets.builder import DATASETS\n",
        "from mmpose.datasets.datasets.base import Kpt2dSviewRgbImgTopDownDataset\n",
        "\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class TopDownCOCOTinyDataset(Kpt2dSviewRgbImgTopDownDataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 ann_file,\n",
        "                 img_prefix,\n",
        "                 data_cfg,\n",
        "                 pipeline,\n",
        "                 dataset_info=None,\n",
        "                 test_mode=False):\n",
        "        super().__init__(\n",
        "            ann_file,\n",
        "            img_prefix,\n",
        "            data_cfg,\n",
        "            pipeline,\n",
        "            dataset_info,\n",
        "            coco_style=False,\n",
        "            test_mode=test_mode)\n",
        "\n",
        "        # flip_pairs, upper_body_ids and lower_body_ids will be used\n",
        "        # in some data augmentations like random flip\n",
        "        self.ann_info['flip_pairs'] = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10],\n",
        "                                       [11, 12], [13, 14], [15, 16]]\n",
        "        self.ann_info['upper_body_ids'] = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
        "        self.ann_info['lower_body_ids'] = (11, 12, 13, 14, 15, 16)\n",
        "\n",
        "        self.ann_info['joint_weights'] = None\n",
        "        self.ann_info['use_different_joint_weights'] = False\n",
        "\n",
        "        self.dataset_name = 'coco_tiny'\n",
        "        self.db = self._get_db()\n",
        "\n",
        "    def _get_db(self):\n",
        "        with open(self.ann_file) as f:\n",
        "            anns = json.load(f)\n",
        "\n",
        "        db = []\n",
        "        for idx, ann in enumerate(anns):\n",
        "            # get image path\n",
        "            image_file = osp.join(self.img_prefix, ann['image_file'])\n",
        "            # get bbox\n",
        "            bbox = ann['bbox']\n",
        "            # get keypoints\n",
        "            keypoints = np.array(\n",
        "                ann['keypoints'], dtype=np.float32).reshape(-1, 3)\n",
        "            num_joints = keypoints.shape[0]\n",
        "            joints_3d = np.zeros((num_joints, 3), dtype=np.float32)\n",
        "            joints_3d[:, :2] = keypoints[:, :2]\n",
        "            joints_3d_visible = np.zeros((num_joints, 3), dtype=np.float32)\n",
        "            joints_3d_visible[:, :2] = np.minimum(1, keypoints[:, 2:3])\n",
        "\n",
        "            sample = {\n",
        "                'image_file': image_file,\n",
        "                'bbox': bbox,\n",
        "                'rotation': 0,\n",
        "                'joints_3d': joints_3d,\n",
        "                'joints_3d_visible': joints_3d_visible,\n",
        "                'bbox_score': 1,\n",
        "                'bbox_id': idx,\n",
        "            }\n",
        "            db.append(sample)\n",
        "\n",
        "        return db\n",
        "\n",
        "    def evaluate(self, results, res_folder=None, metric='PCK', **kwargs):\n",
        "        \"\"\"Evaluate keypoint detection results. The pose prediction results will\n",
        "        be saved in `${res_folder}/result_keypoints.json`.\n",
        "\n",
        "        Note:\n",
        "        batch_size: N\n",
        "        num_keypoints: K\n",
        "        heatmap height: H\n",
        "        heatmap width: W\n",
        "\n",
        "        Args:\n",
        "        results (list(preds, boxes, image_path, output_heatmap))\n",
        "            :preds (np.ndarray[N,K,3]): The first two dimensions are\n",
        "                coordinates, score is the third dimension of the array.\n",
        "            :boxes (np.ndarray[N,6]): [center[0], center[1], scale[0]\n",
        "                , scale[1],area, score]\n",
        "            :image_paths (list[str]): For example, ['Test/source/0.jpg']\n",
        "            :output_heatmap (np.ndarray[N, K, H, W]): model outputs.\n",
        "\n",
        "        res_folder (str, optional): The folder to save the testing\n",
        "                results. If not specified, a temp folder will be created.\n",
        "                Default: None.\n",
        "        metric (str | list[str]): Metric to be performed.\n",
        "            Options: 'PCK', 'NME'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation results for evaluation metric.\n",
        "        \"\"\"\n",
        "        metrics = metric if isinstance(metric, list) else [metric]\n",
        "        allowed_metrics = ['PCK', 'NME']\n",
        "        for metric in metrics:\n",
        "            if metric not in allowed_metrics:\n",
        "                raise KeyError(f'metric {metric} is not supported')\n",
        "\n",
        "        if res_folder is not None:\n",
        "            tmp_folder = None\n",
        "            res_file = osp.join(res_folder, 'result_keypoints.json')\n",
        "        else:\n",
        "            tmp_folder = tempfile.TemporaryDirectory()\n",
        "            res_file = osp.join(tmp_folder.name, 'result_keypoints.json')\n",
        "\n",
        "        kpts = []\n",
        "        for result in results:\n",
        "            preds = result['preds']\n",
        "            boxes = result['boxes']\n",
        "            image_paths = result['image_paths']\n",
        "            bbox_ids = result['bbox_ids']\n",
        "\n",
        "            batch_size = len(image_paths)\n",
        "            for i in range(batch_size):\n",
        "                kpts.append({\n",
        "                    'keypoints': preds[i].tolist(),\n",
        "                    'center': boxes[i][0:2].tolist(),\n",
        "                    'scale': boxes[i][2:4].tolist(),\n",
        "                    'area': float(boxes[i][4]),\n",
        "                    'score': float(boxes[i][5]),\n",
        "                    'bbox_id': bbox_ids[i]\n",
        "                })\n",
        "        kpts = self._sort_and_unique_bboxes(kpts)\n",
        "\n",
        "        self._write_keypoint_results(kpts, res_file)\n",
        "        info_str = self._report_metric(res_file, metrics)\n",
        "        name_value = OrderedDict(info_str)\n",
        "\n",
        "        if tmp_folder is not None:\n",
        "            tmp_folder.cleanup()\n",
        "\n",
        "        return name_value\n",
        "\n",
        "    def _report_metric(self, res_file, metrics, pck_thr=0.3):\n",
        "        \"\"\"Keypoint evaluation.\n",
        "\n",
        "        Args:\n",
        "        res_file (str): Json file stored prediction results.\n",
        "        metrics (str | list[str]): Metric to be performed.\n",
        "            Options: 'PCK', 'NME'.\n",
        "        pck_thr (float): PCK threshold, default: 0.3.\n",
        "\n",
        "        Returns:\n",
        "        dict: Evaluation results for evaluation metric.\n",
        "        \"\"\"\n",
        "        info_str = []\n",
        "\n",
        "        with open(res_file, 'r') as fin:\n",
        "            preds = json.load(fin)\n",
        "        assert len(preds) == len(self.db)\n",
        "\n",
        "        outputs = []\n",
        "        gts = []\n",
        "        masks = []\n",
        "\n",
        "        for pred, item in zip(preds, self.db):\n",
        "            outputs.append(np.array(pred['keypoints'])[:, :-1])\n",
        "            gts.append(np.array(item['joints_3d'])[:, :-1])\n",
        "            masks.append((np.array(item['joints_3d_visible'])[:, 0]) > 0)\n",
        "\n",
        "        outputs = np.array(outputs)\n",
        "        gts = np.array(gts)\n",
        "        masks = np.array(masks)\n",
        "\n",
        "        normalize_factor = self._get_normalize_factor(gts)\n",
        "\n",
        "        if 'PCK' in metrics:\n",
        "            _, pck, _ = keypoint_pck_accuracy(outputs, gts, masks, pck_thr,\n",
        "                                              normalize_factor)\n",
        "            info_str.append(('PCK', pck))\n",
        "\n",
        "        if 'NME' in metrics:\n",
        "            info_str.append(\n",
        "                ('NME', keypoint_nme(outputs, gts, masks, normalize_factor)))\n",
        "\n",
        "        return info_str\n",
        "\n",
        "    @staticmethod\n",
        "    def _write_keypoint_results(keypoints, res_file):\n",
        "        \"\"\"Write results into a json file.\"\"\"\n",
        "\n",
        "        with open(res_file, 'w') as f:\n",
        "            json.dump(keypoints, f, sort_keys=True, indent=4)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sort_and_unique_bboxes(kpts, key='bbox_id'):\n",
        "        \"\"\"sort kpts and remove the repeated ones.\"\"\"\n",
        "        kpts = sorted(kpts, key=lambda x: x[key])\n",
        "        num = len(kpts)\n",
        "        for i in range(num - 1, 0, -1):\n",
        "            if kpts[i][key] == kpts[i - 1][key]:\n",
        "                del kpts[i]\n",
        "\n",
        "        return kpts\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_normalize_factor(gts):\n",
        "        \"\"\"Get inter-ocular distance as the normalize factor, measured as the\n",
        "        Euclidean distance between the outer corners of the eyes.\n",
        "\n",
        "        Args:\n",
        "            gts (np.ndarray[N, K, 2]): Groundtruth keypoint location.\n",
        "\n",
        "        Return:\n",
        "            np.ndarray[N, 2]: normalized factor\n",
        "        \"\"\"\n",
        "\n",
        "        interocular = np.linalg.norm(\n",
        "            gts[:, 0, :] - gts[:, 1, :], axis=1, keepdims=True)\n",
        "        return np.tile(interocular, [1, 2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh05C4mBl_u-"
      },
      "source": [
        "### Create a config file\n",
        "\n",
        "In the next step, we create a config file which configures the model, dataset and runtime settings. More information can be found at [Learn about Configs](https://mmpose.readthedocs.io/en/latest/tutorials/0_config.html). A common practice to create a config file is deriving from a existing one. In this tutorial, we load a config file that trains a HRNet on COCO dataset, and modify it to adapt to the COCOTiny dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-z89qCJoWwL"
      },
      "outputs": [],
      "source": [
        "from mmcv import Config\n",
        "\n",
        "cfg = Config.fromfile(\n",
        "    './configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w32_coco_256x192.py'\n",
        ")\n",
        "\n",
        "# set basic configs\n",
        "cfg.data_root = 'data/coco_tiny'\n",
        "cfg.work_dir = 'work_dirs/hrnet_w32_coco_tiny_256x192'\n",
        "cfg.gpu_ids = range(1)\n",
        "cfg.seed = 0\n",
        "\n",
        "# set log interval\n",
        "cfg.log_config.interval = 1\n",
        "\n",
        "# set evaluation configs\n",
        "cfg.evaluation.interval = 10\n",
        "cfg.evaluation.metric = 'PCK'\n",
        "cfg.evaluation.save_best = 'PCK'\n",
        "\n",
        "# set learning rate policy\n",
        "lr_config = dict(\n",
        "    policy='step',\n",
        "    warmup='linear',\n",
        "    warmup_iters=10,\n",
        "    warmup_ratio=0.001,\n",
        "    step=[17, 35])\n",
        "cfg.total_epochs = 40\n",
        "\n",
        "# set batch size\n",
        "cfg.data.samples_per_gpu = 16\n",
        "cfg.data.val_dataloader = dict(samples_per_gpu=16)\n",
        "cfg.data.test_dataloader = dict(samples_per_gpu=16)\n",
        "\n",
        "# set dataset configs\n",
        "cfg.data.train.type = 'TopDownCOCOTinyDataset'\n",
        "cfg.data.train.ann_file = f'{cfg.data_root}/train.json'\n",
        "cfg.data.train.img_prefix = f'{cfg.data_root}/images/'\n",
        "\n",
        "cfg.data.val.type = 'TopDownCOCOTinyDataset'\n",
        "cfg.data.val.ann_file = f'{cfg.data_root}/val.json'\n",
        "cfg.data.val.img_prefix = f'{cfg.data_root}/images/'\n",
        "\n",
        "cfg.data.test.type = 'TopDownCOCOTinyDataset'\n",
        "cfg.data.test.ann_file = f'{cfg.data_root}/val.json'\n",
        "cfg.data.test.img_prefix = f'{cfg.data_root}/images/'\n",
        "\n",
        "print(cfg.pretty_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQVa6wBDxVSW"
      },
      "source": [
        "### Train and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ5uVkwcxiyx"
      },
      "outputs": [],
      "source": [
        "from mmpose.datasets import build_dataset\n",
        "from mmpose.models import build_posenet\n",
        "from mmpose.apis import train_model\n",
        "import mmcv\n",
        "\n",
        "# build dataset\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "\n",
        "# build model\n",
        "model = build_posenet(cfg.model)\n",
        "\n",
        "# create work_dir\n",
        "mmcv.mkdir_or_exist(cfg.work_dir)\n",
        "\n",
        "# train model\n",
        "train_model(\n",
        "    model, datasets, cfg, distributed=False, validate=True, meta=dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2EWSp1zKoz"
      },
      "source": [
        "Test the trained model. Since the model is trained on a toy dataset coco-tiny, its performance would be as good as the ones in our model zoo. Here we mainly show how to inference and visualize a local model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0rk9eCVzT_D"
      },
      "outputs": [],
      "source": [
        "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
        "                         vis_pose_result, process_mmdet_results)\n",
        "from mmdet.apis import inference_detector, init_detector\n",
        "\n",
        "local_runtime = False\n",
        "\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow  # for image visualization in colab\n",
        "except:\n",
        "    local_runtime = True\n",
        "\n",
        "pose_checkpoint = 'work_dirs/hrnet_w32_coco_tiny_256x192/latest.pth'\n",
        "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "\n",
        "# initialize pose model\n",
        "pose_model = init_pose_model(cfg, pose_checkpoint)\n",
        "# initialize detector\n",
        "det_model = init_detector(det_config, det_checkpoint)\n",
        "\n",
        "img = 'tests/data/coco/000000196141.jpg'\n",
        "\n",
        "# inference detection\n",
        "mmdet_results = inference_detector(det_model, img)\n",
        "\n",
        "# extract person (COCO_ID=1) bounding boxes from the detection results\n",
        "person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
        "\n",
        "# inference pose\n",
        "pose_results, returned_outputs = inference_top_down_pose_model(\n",
        "    pose_model,\n",
        "    img,\n",
        "    person_results,\n",
        "    bbox_thr=0.3,\n",
        "    format='xyxy',\n",
        "    dataset='TopDownCocoDataset')\n",
        "\n",
        "# show pose estimation results\n",
        "vis_result = vis_pose_result(\n",
        "    pose_model,\n",
        "    img,\n",
        "    pose_results,\n",
        "    kpt_score_thr=0.,\n",
        "    dataset='TopDownCocoDataset',\n",
        "    show=False)\n",
        "\n",
        "# reduce image size\n",
        "vis_result = cv2.resize(vis_result, dsize=None, fx=0.5, fy=0.5)\n",
        "\n",
        "if local_runtime:\n",
        "    from IPython.display import Image, display\n",
        "    import tempfile\n",
        "    import os.path as osp\n",
        "    import cv2\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file_name = osp.join(tmpdir, 'pose_results.png')\n",
        "        cv2.imwrite(file_name, vis_result)\n",
        "        display(Image(file_name))\n",
        "else:\n",
        "    cv2_imshow(vis_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MMPose_Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "46cabf725503616575ee9df11fae44e77863ccc5fe9a7400abcc9d5976385eac"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('pt1.9': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}